"""
SAAH-WM Baseline ç¬¬äºŒæ­¥ - ä¸»ç¨‹åºå…¥å£

æœ¬ç¨‹åºå®ç°å®Œæ•´çš„æ°´å°ç¼–ç å™¨/è§£ç å™¨è®­ç»ƒæµç¨‹ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python main.py --config configs/train_config.yaml
    python main.py --config configs/train_config.yaml --resume checkpoints/checkpoint.pth
    python main.py --config configs/train_config.yaml --test_only

åŠŸèƒ½ç‰¹ç‚¹ï¼š
- æ”¯æŒä»é…ç½®æ–‡ä»¶åŠ è½½è®­ç»ƒå‚æ•°
- æ”¯æŒæ–­ç‚¹ç»­è®­
- æ”¯æŒä»…æµ‹è¯•æ¨¡å¼
- å®Œæ•´çš„æ—¥å¿—è®°å½•å’Œæ€§èƒ½ç›‘æ§
- è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡
"""

import argparse
import os
import sys
import yaml
import torch
import logging
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.trainer import WatermarkTrainer
from data.coco_dataloader import create_dataloader
from utils.logger_config import setup_training_logger


def load_config(config_path: str) -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: é…ç½®å­—å…¸
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
        
    return config


def setup_device(config: Dict[str, Any]) -> str:
    """
    è®¾ç½®è®­ç»ƒè®¾å¤‡
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
        
    Returns:
        str: è®¾å¤‡åç§°
    """
    if config.get('device', 'cuda') == 'cuda' and torch.cuda.is_available():
        device = 'cuda'
        print(f"ä½¿ç”¨GPUè®­ç»ƒ: {torch.cuda.get_device_name()}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        device = 'cpu'
        print("ä½¿ç”¨CPUè®­ç»ƒ")
        
    return device


def create_dataloaders(config: Dict[str, Any]) -> tuple:
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
        
    Returns:
        tuple: (train_dataloader, val_dataloader)
    """
    dataset_config = config['dataset']
    
    # è®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_dataloader = create_dataloader(
        coco_path=dataset_config['coco_path'],
        batch_size=dataset_config['batch_size'],
        num_workers=dataset_config['num_workers'],
        pin_memory=dataset_config['pin_memory'],
        shuffle=True,
        split='train',
        image_size=dataset_config['image_size'],
        crop_size=dataset_config['crop_size'],
        use_step1_modules=True  # ä½¿ç”¨ç¬¬ä¸€æ­¥æ¨¡å—
    )
    
    # éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨éƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼‰
    val_dataloader = create_dataloader(
        coco_path=dataset_config['coco_path'],
        batch_size=dataset_config['batch_size'],
        num_workers=dataset_config['num_workers'],
        pin_memory=dataset_config['pin_memory'],
        shuffle=False,
        split='val',
        image_size=dataset_config['image_size'],
        crop_size=dataset_config['crop_size'],
        use_step1_modules=True
    )
    
    return train_dataloader, val_dataloader


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='SAAH-WM Baseline ç¬¬äºŒæ­¥è®­ç»ƒç¨‹åº')
    parser.add_argument('--config', type=str, required=True,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--test_only', action='store_true',
                       help='ä»…è¿›è¡Œæµ‹è¯•ï¼Œä¸è®­ç»ƒ')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        print("ğŸ”§ åŠ è½½é…ç½®æ–‡ä»¶...")
        config = load_config(args.config)
        
        # è®¾ç½®è°ƒè¯•æ¨¡å¼
        if args.debug or config.get('debug', {}).get('enabled', False):
            config['logging']['level'] = 'DEBUG'
            print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
        
        # è®¾ç½®æ—¥å¿—
        training_logger = setup_training_logger(
            log_dir=config['logging']['log_dir'],
            log_level=config['logging']['level']
        )
        
        print("ğŸ“ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # è®¾ç½®è®¾å¤‡
        device = setup_device(config)
        
        # è®¾ç½®éšæœºç§å­
        if 'seed' in config:
            torch.manual_seed(config['seed'])
            if torch.cuda.is_available():
                torch.cuda.manual_seed(config['seed'])
            print(f"ğŸ² éšæœºç§å­è®¾ç½®ä¸º: {config['seed']}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        train_dataloader, val_dataloader = create_dataloaders(config)
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataloader.dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(val_dataloader.dataset)}")
        print(f"æ‰¹æ¬¡å¤§å°: {config['dataset']['batch_size']}")
        print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_dataloader)}")
        print(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_dataloader)}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        print("ğŸ¤– åˆå§‹åŒ–è®­ç»ƒå™¨...")
        trainer = WatermarkTrainer(config, device)
        
        # æ‰“å°æ¨¡å‹æ‘˜è¦
        print("\n" + trainer.get_model_summary())
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.resume:
            print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume}")
            trainer.load_checkpoint(args.resume)
        
        if args.test_only:
            # ä»…æµ‹è¯•æ¨¡å¼
            print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
            val_metrics = trainer.validate_epoch(val_dataloader, training_logger)
            
            print("\næµ‹è¯•ç»“æœ:")
            for key, value in val_metrics.items():
                if 'accuracy' in key or 'rate' in key:
                    print(f"  {key}: {value:.4f}")
                elif 'loss' in key:
                    print(f"  {key}: {value:.6f}")
                else:
                    print(f"  {key}: {value:.2f}")
                    
        else:
            # è®­ç»ƒæ¨¡å¼
            print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
            
            # æ£€æŸ¥å¿«é€Ÿè¿è¡Œæ¨¡å¼
            if config.get('debug', {}).get('fast_run', False):
                print("âš¡ å¿«é€Ÿè¿è¡Œæ¨¡å¼å·²å¯ç”¨")
                # é™åˆ¶æ•°æ®é›†å¤§å°
                fast_samples = config['debug']['fast_run_samples']
                train_dataloader.dataset.image_files = train_dataloader.dataset.image_files[:fast_samples]
                val_dataloader.dataset.image_files = val_dataloader.dataset.image_files[:fast_samples//10]
                print(f"æ•°æ®é›†å·²é™åˆ¶ä¸º {fast_samples} ä¸ªè®­ç»ƒæ ·æœ¬")
            
            # å¼€å§‹è®­ç»ƒ
            best_metrics = trainer.train(
                train_dataloader=train_dataloader,
                val_dataloader=val_dataloader,
                training_logger=training_logger
            )
            
            print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
            print("æœ€ä½³æ€§èƒ½:")
            for key, value in best_metrics.items():
                if key != 'epoch':
                    if 'accuracy' in key or 'rate' in key:
                        print(f"  {key}: {value:.4f}")
                    elif 'loss' in key:
                        print(f"  {key}: {value:.6f}")
                    else:
                        print(f"  {key}: {value:.2f}")
            
            # æ£€æŸ¥æ€§èƒ½é˜ˆå€¼
            thresholds = config['checkpoints']['performance_thresholds']
            print("\nğŸ¯ æ€§èƒ½é˜ˆå€¼æ£€æŸ¥:")
            
            for metric, threshold in thresholds.items():
                if metric in best_metrics:
                    value = best_metrics[metric]
                    status = "âœ… è¾¾æ ‡" if value >= threshold else "âŒ æœªè¾¾æ ‡"
                    print(f"  {metric}: {value:.4f} (é˜ˆå€¼: {threshold}) {status}")
            
            # è¾“å‡ºæ¨¡å‹æ–‡ä»¶ä½ç½®
            models_dir = os.path.join(config['checkpoints']['save_dir'], 'best_models')
            if os.path.exists(models_dir):
                print(f"\nğŸ’¾ æœ€ä½³æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: {models_dir}")
                print("åŒ…å«ä»¥ä¸‹æ–‡ä»¶:")
                for filename in ['fragile_encoder.pth', 'robust_encoder.pth', 
                               'robust_decoder.pth', 'fragile_decoder.pth']:
                    filepath = os.path.join(models_dir, filename)
                    if os.path.exists(filepath):
                        size_mb = os.path.getsize(filepath) / (1024 * 1024)
                        print(f"  - {filename} ({size_mb:.1f} MB)")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        sys.exit(1)


if __name__ == "__main__":
    main()
