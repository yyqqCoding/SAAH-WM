"""
SAAH-WM Baseline ç¬¬äºŒæ­¥ - æ—¥å¿—é…ç½®æ¨¡å—

æœ¬æ¨¡å—æä¾›å®Œæ•´çš„æ—¥å¿—ç³»ç»Ÿé…ç½®ï¼Œæ”¯æŒï¼š
- æ–‡ä»¶æ—¥å¿—å’Œæ§åˆ¶å°æ—¥å¿—
- ä¸åŒçº§åˆ«çš„æ—¥å¿—è®°å½•
- è®­ç»ƒè¿›åº¦ç›‘æ§
- é”™è¯¯å¼‚å¸¸è¿½è¸ª
- ä¸­æ–‡å‹å¥½çš„æ—¥å¿—æ ¼å¼
"""

import logging
import logging.handlers
import os
import sys
from datetime import datetime
from typing import Optional, Dict, Any

# å°è¯•å¯¼å…¥colorlogï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ ‡å‡†æ—¥å¿—
try:
    import colorlog
    HAS_COLORLOG = True
except ImportError:
    HAS_COLORLOG = False


def setup_logger(name: str = 'saah_wm', 
                log_dir: str = 'logs',
                log_level: str = 'INFO',
                console_output: bool = True,
                file_output: bool = True,
                max_file_size: int = 10 * 1024 * 1024,  # 10MB
                backup_count: int = 5) -> logging.Logger:
    """
    è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        name (str): æ—¥å¿—å™¨åç§°ï¼Œé»˜è®¤'saah_wm'
        log_dir (str): æ—¥å¿—æ–‡ä»¶ç›®å½•ï¼Œé»˜è®¤'logs'
        log_level (str): æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤'INFO'
        console_output (bool): æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé»˜è®¤True
        file_output (bool): æ˜¯å¦è¾“å‡ºåˆ°æ–‡ä»¶ï¼Œé»˜è®¤True
        max_file_size (int): å•ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤10MB
        backup_count (int): ä¿ç•™çš„æ—¥å¿—æ–‡ä»¶æ•°é‡ï¼Œé»˜è®¤5
        
    Returns:
        logging.Logger: é…ç½®å¥½çš„æ—¥å¿—å™¨
    """
    
    # åˆ›å»ºæ—¥å¿—å™¨
    logger = logging.getLogger(name)
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
    logger.handlers.clear()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    if file_output and not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    
    # æ—¥å¿—æ ¼å¼
    detailed_formatter = logging.Formatter(
        fmt='%(asctime)s | %(name)s | %(levelname)s | %(filename)s:%(lineno)d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    simple_formatter = logging.Formatter(
        fmt='%(asctime)s | %(levelname)s | %(message)s',
        datefmt='%H:%M:%S'
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨ï¼ˆå¸¦é¢œè‰²ï¼‰
    if console_output:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)

        # å½©è‰²æ ¼å¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if HAS_COLORLOG:
            color_formatter = colorlog.ColoredFormatter(
                fmt='%(log_color)s%(asctime)s | %(levelname)s | %(message)s%(reset)s',
                datefmt='%H:%M:%S',
                log_colors={
                    'DEBUG': 'cyan',
                    'INFO': 'green',
                    'WARNING': 'yellow',
                    'ERROR': 'red',
                    'CRITICAL': 'red,bg_white',
                }
            )
            console_handler.setFormatter(color_formatter)
        else:
            # ä½¿ç”¨ç®€å•æ ¼å¼
            console_handler.setFormatter(simple_formatter)

        logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    if file_output:
        # ä¸»æ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = os.path.join(log_dir, f'training_{timestamp}.log')
        
        file_handler = logging.handlers.RotatingFileHandler(
            log_filename,
            maxBytes=max_file_size,
            backupCount=backup_count,
            encoding='utf-8'
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(detailed_formatter)
        logger.addHandler(file_handler)
        
        # é”™è¯¯æ—¥å¿—æ–‡ä»¶
        error_filename = os.path.join(log_dir, f'error_{timestamp}.log')
        error_handler = logging.FileHandler(error_filename, encoding='utf-8')
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(detailed_formatter)
        logger.addHandler(error_handler)
    
    # è®°å½•åˆå§‹åŒ–ä¿¡æ¯
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    logger.info(f"æ—¥å¿—çº§åˆ«: {log_level}")
    logger.info(f"æ§åˆ¶å°è¾“å‡º: {console_output}")
    logger.info(f"æ–‡ä»¶è¾“å‡º: {file_output}")
    if file_output:
        logger.info(f"æ—¥å¿—ç›®å½•: {log_dir}")
        logger.info(f"ä¸»æ—¥å¿—æ–‡ä»¶: {log_filename}")
        logger.info(f"é”™è¯¯æ—¥å¿—æ–‡ä»¶: {error_filename}")
    
    return logger


class TrainingLogger:
    """
    è®­ç»ƒä¸“ç”¨æ—¥å¿—å™¨
    
    æä¾›è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸“é—¨æ—¥å¿—è®°å½•åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - è®­ç»ƒè¿›åº¦è®°å½•
    - æŸå¤±å’ŒæŒ‡æ ‡è®°å½•
    - æ¨¡å‹çŠ¶æ€è®°å½•
    - é”™è¯¯å’Œå¼‚å¸¸å¤„ç†
    """
    
    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.epoch_start_time = None
        self.batch_start_time = None
        
    def log_training_start(self, config: Dict[str, Any]):
        """è®°å½•è®­ç»ƒå¼€å§‹"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ SAAH-WM Baseline ç¬¬äºŒæ­¥è®­ç»ƒå¼€å§‹")
        self.logger.info("=" * 80)
        
        # è®°å½•é…ç½®ä¿¡æ¯
        self.logger.info("è®­ç»ƒé…ç½®:")
        for key, value in config.items():
            if isinstance(value, dict):
                self.logger.info(f"  {key}:")
                for sub_key, sub_value in value.items():
                    self.logger.info(f"    {sub_key}: {sub_value}")
            else:
                self.logger.info(f"  {key}: {value}")
                
    def log_epoch_start(self, epoch: int, total_epochs: int):
        """è®°å½•epochå¼€å§‹"""
        self.epoch_start_time = datetime.now()
        self.logger.info("-" * 60)
        self.logger.info(f"ğŸ“ˆ Epoch {epoch}/{total_epochs} å¼€å§‹")
        self.logger.info(f"å¼€å§‹æ—¶é—´: {self.epoch_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
    def log_batch_progress(self, batch_idx: int, total_batches: int, 
                          loss_details: Dict[str, float], 
                          learning_rate: float,
                          log_interval: int = 100):
        """è®°å½•æ‰¹æ¬¡è¿›åº¦"""
        if batch_idx % log_interval == 0:
            progress = batch_idx / total_batches * 100
            
            # æ„å»ºæŸå¤±ä¿¡æ¯å­—ç¬¦ä¸²
            loss_str = " | ".join([f"{k}: {v:.6f}" for k, v in loss_details.items()])
            
            self.logger.info(
                f"Batch {batch_idx:4d}/{total_batches} ({progress:5.1f}%) | "
                f"LR: {learning_rate:.2e} | {loss_str}"
            )
            
    def log_epoch_end(self, epoch: int, train_metrics: Dict[str, float], 
                     val_metrics: Optional[Dict[str, float]] = None):
        """è®°å½•epochç»“æŸ"""
        if self.epoch_start_time:
            epoch_time = datetime.now() - self.epoch_start_time
            self.logger.info(f"â±ï¸  Epoch {epoch} è€—æ—¶: {epoch_time}")
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        self.logger.info("ğŸ“Š è®­ç»ƒæŒ‡æ ‡:")
        for key, value in train_metrics.items():
            if 'accuracy' in key or 'rate' in key:
                self.logger.info(f"  {key}: {value:.4f}")
            elif 'loss' in key:
                self.logger.info(f"  {key}: {value:.6f}")
            else:
                self.logger.info(f"  {key}: {value:.2f}")
        
        # è®°å½•éªŒè¯æŒ‡æ ‡
        if val_metrics:
            self.logger.info("ğŸ” éªŒè¯æŒ‡æ ‡:")
            for key, value in val_metrics.items():
                if 'accuracy' in key or 'rate' in key:
                    self.logger.info(f"  {key}: {value:.4f}")
                elif 'loss' in key:
                    self.logger.info(f"  {key}: {value:.6f}")
                else:
                    self.logger.info(f"  {key}: {value:.2f}")
                    
    def log_model_save(self, epoch: int, filepath: str, metrics: Dict[str, float]):
        """è®°å½•æ¨¡å‹ä¿å­˜"""
        self.logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ (Epoch {epoch})")
        self.logger.info(f"ä¿å­˜è·¯å¾„: {filepath}")
        self.logger.info(f"æ€§èƒ½æŒ‡æ ‡: {metrics}")
        
    def log_best_performance(self, epoch: int, metrics: Dict[str, float]):
        """è®°å½•æœ€ä½³æ€§èƒ½"""
        self.logger.info("ğŸ† å‘ç°æ–°çš„æœ€ä½³æ€§èƒ½!")
        self.logger.info(f"Epoch: {epoch}")
        for key, value in metrics.items():
            if 'accuracy' in key or 'rate' in key:
                self.logger.info(f"  {key}: {value:.4f}")
            elif 'loss' in key:
                self.logger.info(f"  {key}: {value:.6f}")
            else:
                self.logger.info(f"  {key}: {value:.2f}")
                
    def log_validation_start(self, num_samples: int):
        """è®°å½•éªŒè¯å¼€å§‹"""
        self.logger.info(f"ğŸ” å¼€å§‹éªŒè¯ (æ ·æœ¬æ•°: {num_samples})")
        
    def log_validation_end(self, metrics: Dict[str, float]):
        """è®°å½•éªŒè¯ç»“æŸ"""
        self.logger.info("âœ… éªŒè¯å®Œæˆ")
        
        # æ£€æŸ¥æ€§èƒ½é˜ˆå€¼
        thresholds = {
            'psnr': 38.0,
            'ssim': 0.95,
            'bit_accuracy': 0.995
        }
        
        self.logger.info("ğŸ¯ æ€§èƒ½é˜ˆå€¼æ£€æŸ¥:")
        for metric, threshold in thresholds.items():
            if metric in metrics:
                value = metrics[metric]
                status = "âœ… è¾¾æ ‡" if value >= threshold else "âŒ æœªè¾¾æ ‡"
                self.logger.info(f"  {metric}: {value:.4f} (é˜ˆå€¼: {threshold}) {status}")
                
    def log_error(self, error: Exception, context: str = ""):
        """è®°å½•é”™è¯¯"""
        self.logger.error(f"âŒ å‘ç”Ÿé”™è¯¯ {context}")
        self.logger.error(f"é”™è¯¯ç±»å‹: {type(error).__name__}")
        self.logger.error(f"é”™è¯¯ä¿¡æ¯: {str(error)}")
        self.logger.exception("è¯¦ç»†é”™è¯¯å †æ ˆ:")
        
    def log_warning(self, message: str):
        """è®°å½•è­¦å‘Š"""
        self.logger.warning(f"âš ï¸  {message}")
        
    def log_training_end(self, total_time: str, best_metrics: Dict[str, float]):
        """è®°å½•è®­ç»ƒç»“æŸ"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        self.logger.info(f"æ€»è€—æ—¶: {total_time}")
        self.logger.info("æœ€ä½³æ€§èƒ½:")
        for key, value in best_metrics.items():
            if 'accuracy' in key or 'rate' in key:
                self.logger.info(f"  {key}: {value:.4f}")
            elif 'loss' in key:
                self.logger.info(f"  {key}: {value:.6f}")
            else:
                self.logger.info(f"  {key}: {value:.2f}")
        self.logger.info("=" * 80)


def setup_training_logger(log_dir: str = 'logs', 
                         log_level: str = 'INFO') -> TrainingLogger:
    """
    è®¾ç½®è®­ç»ƒä¸“ç”¨æ—¥å¿—å™¨
    
    Args:
        log_dir (str): æ—¥å¿—ç›®å½•
        log_level (str): æ—¥å¿—çº§åˆ«
        
    Returns:
        TrainingLogger: è®­ç»ƒæ—¥å¿—å™¨
    """
    base_logger = setup_logger(
        name='saah_wm_training',
        log_dir=log_dir,
        log_level=log_level,
        console_output=True,
        file_output=True
    )
    
    return TrainingLogger(base_logger)


if __name__ == "__main__":
    # æµ‹è¯•æ—¥å¿—ç³»ç»Ÿ
    
    # åŸºç¡€æ—¥å¿—å™¨æµ‹è¯•
    print("æµ‹è¯•åŸºç¡€æ—¥å¿—å™¨:")
    logger = setup_logger(log_level='DEBUG')
    
    logger.debug("è¿™æ˜¯è°ƒè¯•ä¿¡æ¯")
    logger.info("è¿™æ˜¯æ™®é€šä¿¡æ¯")
    logger.warning("è¿™æ˜¯è­¦å‘Šä¿¡æ¯")
    logger.error("è¿™æ˜¯é”™è¯¯ä¿¡æ¯")
    
    # è®­ç»ƒæ—¥å¿—å™¨æµ‹è¯•
    print("\næµ‹è¯•è®­ç»ƒæ—¥å¿—å™¨:")
    training_logger = setup_training_logger()
    
    # æ¨¡æ‹Ÿè®­ç»ƒé…ç½®
    config = {
        'model': {
            'latent_channels': 4,
            'hidden_dim': 256
        },
        'training': {
            'batch_size': 4,
            'learning_rate': 1e-4,
            'num_epochs': 100
        }
    }
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    training_logger.log_training_start(config)
    
    training_logger.log_epoch_start(1, 100)
    
    # æ¨¡æ‹Ÿæ‰¹æ¬¡è¿›åº¦
    loss_details = {
        'total_loss': 0.1234,
        'image_loss': 0.0456,
        'robust_loss': 0.0678,
        'fragile_loss': 0.0100
    }
    training_logger.log_batch_progress(100, 1000, loss_details, 1e-4)
    
    # æ¨¡æ‹Ÿepochç»“æŸ
    train_metrics = {
        'psnr': 39.5,
        'ssim': 0.96,
        'bit_accuracy': 0.997,
        'total_loss': 0.0987
    }
    
    val_metrics = {
        'psnr': 38.2,
        'ssim': 0.95,
        'bit_accuracy': 0.995,
        'total_loss': 0.1123
    }
    
    training_logger.log_epoch_end(1, train_metrics, val_metrics)
    
    # æ¨¡æ‹ŸéªŒè¯è¿‡ç¨‹
    training_logger.log_validation_start(1000)
    training_logger.log_validation_end(val_metrics)
    
    # æ¨¡æ‹Ÿæœ€ä½³æ€§èƒ½
    training_logger.log_best_performance(1, val_metrics)
    
    # æ¨¡æ‹Ÿæ¨¡å‹ä¿å­˜
    training_logger.log_model_save(1, "checkpoints/best_model.pth", val_metrics)
    
    # æ¨¡æ‹Ÿé”™è¯¯
    try:
        raise ValueError("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é”™è¯¯")
    except Exception as e:
        training_logger.log_error(e, "åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­")
    
    # æ¨¡æ‹Ÿè­¦å‘Š
    training_logger.log_warning("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è­¦å‘Š")
    
    # æ¨¡æ‹Ÿè®­ç»ƒç»“æŸ
    training_logger.log_training_end("2å°æ—¶30åˆ†é’Ÿ", val_metrics)
    
    print("\næ—¥å¿—æµ‹è¯•å®Œæˆ! è¯·æ£€æŸ¥logsç›®å½•ä¸­çš„æ—¥å¿—æ–‡ä»¶ã€‚")
