# SAAH-WM：语义感知与自适应分层水印技术

本文档详细阐述了一种名为“语义感知与自适应分层水印（Semantic-Aware Adaptive Hierarchical Watermarking, SAAH-WM）”的新型扩散模型水印技术。该技术旨在解决当前AIGC（人工智能生成内容）领域在图像版权保护、篡改检测与定位方面面临的核心挑战，通过引入语义理解能力，实现一个更智能、更安全、更高性能的盲水印系统。

## 一、 核心创新点

SAAH-WM的核心思想是将水印从一个被动的、静态的标记，转变为一个主动的、能够理解图像内容并随之智能变化的守护系统。其创新性主要体现在以下三个层面：

1. **水印的“身份”：由语义驱动，一图一印**
   - **创新详述：** 摒弃了所有图像共享固定或伪随机水印模式的传统做法。SAAH-WM的基础水印图样是由生成该图像的**文本提示（Prompt）的语义向量唯一且确定性地生成**。这意味着，只要创作意图（文本）不同，其水印“基因”就完全不同。
   - **解决的问题：** 从根本上杜绝了攻击者通过分析多个样本来寻找通用规律并设计针对性攻击的可能性，极大地提升了水印的安全性。
2. **水印的“策略”：自适应分层，智能攻防**
   - **创新详述：** 本方法利用扩散模型生成过程中自然产生的**交叉注意力图谱（Cross-Attention Maps）**作为“语义重要性地图”，来指导一个统一的水印在图像的不同空间区域展现出不同的防御特性。
     - 在**关键区域**（如人脸、商标），水印表现为**高度脆弱**，以实现像素级的精准篡改定位。
     - 在**背景区域**（如天空、草地），水印则表现为**高度鲁棒**，以确保其承载的核心信息（语义指纹和版权）在各类攻击下仍能被可靠提取。
   - **解决的问题：** 超越了现有“双水印”框架的物理叠加，实现了单一水印攻防属性的有机融合与智能调度，旨在完美平衡篡改定位精度、信息提取鲁棒性与图像保真度之间的“三角矛盾”。
3. **水印的“解码”：指纹自嵌入，实现真盲提**
   - **创新详述：** 为了在不知道原始Prompt的情况下验证一个由Prompt动态生成的水印，SAAH-WM开创性地将生成基础水印所需的“密钥”——即**文本提示的压缩版“语义指纹”**——作为元数据的一部分，安全地**嵌入到水印的鲁棒层中**。
   - **解决的问题：** 构建了一个信息自洽的闭环系统。在验证时，解码器先从图像中恢复出“语义指纹”，再用它在本地实时生成用于比对的基准水印，从而实现了**动态水印的完全盲提取**，使系统具备了极高的独立性与通用性。

## 二、 详细实现蓝图

以下将详细拆解该方法的两大核心阶段：**水印嵌入端**和**水印提取与篡改分析端**。

### **阶段一：水印嵌入端 (Embedding End)**

此阶段在扩散模型生成图像的同时，完成一个高度定制化水印的嵌入。

#### **模块 1: 语义指纹生成与压缩**

- **目标：** 将用户的文本创作意图，这一非结构化的自然语言信息，转化为一个紧凑、固定长度、可作为加密密钥和嵌入载荷的二进制“语义指纹”。
- **详细步骤：**
  1. **提取语义向量 `c`：**
     - **输入：** 用户提供的文本提示 `Prompt` (例如: "一只戴着墨镜的宇航员猫坐在月球上")。
     - **操作：** 调用扩散模型（如Stable Diffusion）内部的 **CLIP Text Encoder**，将 `Prompt` 编码为一个768维（或更高维）的浮点数向量 `c`。这个向量 `c` 精准地捕捉了文本的深层语义。选择CLIP是因为其训练目标就是对齐文本和图像的语义空间，使得这个向量 `c` 成为图像内容的最佳数学描述。
  2. **训练VQ-VAE压缩模型：**
     - **目的：** 离线准备一个能高效压缩语义向量 `c` 的模型。直接嵌入768维的浮点向量是不现实的，会占用过多容量。
     - **数据准备：** 收集一个大规模的文本-图像对数据集（如LAION-5B的一个子集），并提取所有文本对应的CLIP语义向量 `c` 作为训练数据。
     - **模型结构：** 构建一个轻量级的**矢量量化自编码器（VQ-VAE）**。它包含一个编码器（将768维`c`映射到低维连续空间）、一个码本（Codebook，包含例如1024个可学习的码向量）和一个解码器（从码向量索引重构`c`）。
     - **训练过程：** 训练的目标是最小化重构损失（原始`c`与解码器输出的`c'`之间的差异）和码本的承诺损失（Commitment Loss），确保压缩过程信息损失最小。
  3. **生成比特流 `c_bits`：**
     - **操作：** 在实时嵌入时，将步骤1中得到的向量 `c` 输入到训练好的VQ-VAE编码器中，它会找到码本中最接近的码向量，并输出该码向量的索引。例如，如果码本大小为1024，则索引范围是0-1023，可以用10个比特来表示。如果为了更精确的表示，可以输出一个索引序列（例如，一个`c`向量用4个码向量来组合表示），从而得到一个更长的比特流 `c_bits`（例如，4 * 10 = 40 bits，可以根据需求调整）。我们假设最终目标长度为256 bits，以平衡表示精度和嵌入负载。
- **可参考的技术/论文：**
  - **CLIP Text Encoder：** 直接利用 **Stable Diffusion** 开源代码中的实现，无需改动。
  - **VQ-VAE：** 可参考其原始论文 (`Neural Discrete Representation Learning` by van den Oord et al.) 或在GitHub上寻找成熟的PyTorch实现作为基础。需要针对CLIP的语义向量数据集进行专门训练。

#### **模块 2: 语义重要性分析**

- **目标：** 识别图像中哪些区域是内容的“主角”，哪些是“背景”，并生成一张灰度指导图，其像素值与语义重要性成正比。
- **详细步骤：**
  1. **提取交叉注意力图谱：**
     - **时机：** 在扩散模型的U-Net执行每一步去噪（Denoising Step）时，特别是在结构逐渐清晰的中间到后期步骤。
     - **操作：** 访问U-Net中每个**交叉注意力层（Cross-Attention Layers）**的输出。这些层负责将文本的语义信息与图像的空间特征对齐。其输出的注意力图谱（通常是 HxW 维度的矩阵）直观地显示了图像每个像素对Prompt中每个单词（Token）的“关注度”。
  2. **生成语义重要性掩码 `M_sem`：**
     - **操作：**
       - **Token识别：** 解析Prompt，识别出代表核心实体的名词Token（例如，"cat", "sunglasses", "moon"）。
       - **图谱聚合：** 对于每个核心名词Token，将其在不同去噪步骤、不同注意力头的对应图谱进行平均或加权平均，以获得一个稳定、清晰的关注度图。
       - **加权融合与归一化：** 将所有核心名词的关注度图进行加权融合。可以给予更核心的词（如"cat"）更高的权重。融合后，进行高斯模糊以平滑边缘，并进行归一化处理，生成最终的单通道灰度掩码 `M_sem`。
     - **结果：** 在 `M_sem` 中，猫和月球对应的区域像素值会很高（接近1），墨镜区域次之，而广阔的宇宙背景区域的像素值会很低（接近0）。这张图为后续的自适应嵌入提供了精细的逐像素指导。
- **可参考的技术/论文：**
  - **交叉注意力图谱提取：** **`ControlNet`** 的工作对此有深入应用。其开源代码是学习如何准确提取和可视化这些图谱的最佳实践。**`Prompt-to-Prompt`** 论文也详细探讨了如何操作注意力图。

#### **模块 3: 自适应分层水印嵌入**

- **目标：** 将包含“语义指纹”和“版权信息”的整合水印，根据 `M_sem` 的指导，以不同强度和特性智能地嵌入图像。这是一个精细的平衡艺术。
- **详细步骤：**
  1. **信息整合：** 将 `c_bits` 和 `copyright_bits` 拼接成一个完整的信息包 `M`。可选择性地加入BCH等前向纠错码（Forward Error Correction），以增加解码的容错性。
  2. **采用串行嵌入框架 (参考 `EditGuard`)：**
     - **逻辑顺序：** 先嵌入脆弱的定位水印，因为它对图像的扰动更小；后嵌入鲁棒的元数据水印，其网络需要学习在已被轻微扰动的图像上进行稳健嵌入。
     - **第一步：嵌入脆弱的定位水印 `W_loc`**
       - **网络结构：** 使用一个**可逆神经网络（INN）**，其正向过程用于嵌入，逆向过程用于提取。这是实现脆弱性的关键，因为INN的逆过程对输入的变化高度敏感。
       - **自适应调制：** 选择一个固定的、高频丰富的自然图像作为基础定位水印图样 `W_loc_pattern`。在执行嵌入时，实际加入的扰动量由 `M_sem` 控制。例如，`P_loc = W_loc_pattern * (1 - M_sem)`。这样，在重要区域（`M_sem`≈1），扰动几乎为零，水印极度脆弱；在背景区域（`M_sem`≈0），扰动被完整加入，形成一个相对鲁棒的“锚点”。
     - **第二步：嵌入鲁棒的元数据水印 `M`**
       - **网络结构：** 使用一个鲁棒的深度水印网络，如U-Net结构，它擅长在多尺度上嵌入信息。
       - **自适应调制：** 在背景区域（`M_sem`≈0），采用更强的鲁棒性策略。例如，可以将信息包 `M` 进行多次重复嵌入，或者使用更低码率的纠错码，并增加嵌入的能量（扰动幅度）。在重要区域（`M_sem`≈1），则采用极轻微的嵌入或不嵌入，以最大限度保护画质，并将鲁棒性任务完全交给背景区域。
- **可参考的技术/论文：**
  - **可逆神经网络 (INN)：** **`EditGuard`** 及其引用的 **`RIIS`** 等I2I隐写术论文是实现此部分的基础。
  - **鲁棒水印网络：** **`OmniGuard`**, **`TrustMark`**, **`GenPTW`** 等工作展示了如何构建能够抵抗多种降级的鲁棒水印编码器。

#### **模块 4: 联合训练与优化**

- **目标：** 通过设计精巧的损失函数和模拟真实攻击的训练策略，使整个系统能够在保真度、鲁棒性、定位精度和信息提取准确率之间达到最佳平衡。
- **详细步骤：**
  1. **设计多目标损失函数：**
     - **保真度损失：** 综合使用像素级的L2损失（保证基础一致性）、感知级的LPIPS损失（保证视觉相似性）、GAN对抗损失（提升细节真实感），以及 **`GenPTW`** 提出的 **JND引导损失**（在人眼不敏感的纹理区域隐藏更多扰动）。
     - **信息提取损失：** 对提取出的信息包 `M_extracted` 和原始 `M` 计算二元交叉熵损失（BCE Loss）。
     - **篡改定位损失：** 对最终输出的篡改掩码 `M_loc` 和真实掩码 `M_gt` 计算损失。推荐使用 **`OmniGuard`** 采用的 **BCE Loss + Edge Loss** 的组合，后者能惩罚定位边缘的模糊，使结果更锐利。
  2. **实施强鲁棒性训练：**
     - 在训练循环中，以一定概率对嵌入水印后的图像 `I_con` 应用一个强大的**失真模拟层**。
     - **强烈推荐**借鉴 **`OmniGuard`** 的**轻量级AIGC编辑模拟层**，它高效地模拟了全局（VAE重构）和局部（Inpainting/移除）AIGC编辑。
     - **构建降级链（Degradation Chain）：** 为了模拟真实世界，不能只应用单一攻击。应随机组合多种攻击，形成攻击链。例如：`图像 -> Inpainting -> JPEG压缩(Q=75) -> 亮度降低10% -> 最终输入`。这种复杂的攻击链能极大地提升模型的泛化能力。
- **可参考的技术/论文：**
  - **损失函数与失真模拟层：** **`OmniGuard`** 和 **`GenPTW`** 是最直接、最先进的参考。

### **阶段二：提取与篡改分析端 (Blind Extraction & Analysis)**

此阶段在完全“盲”状态下，从单张图片中提取信息，并完成精准的篡改分析。

#### **模块 5: 盲提取与水印重构**

- **目标：** 在不知道任何原始信息的情况下，从待测图像中恢复出所有必需的验证信息，实现信息的自洽闭环。
- **详细步骤：**
  1. **提取元数据 `M_extracted`：** 将待测图像 `I_rec` 输入到训练好的**鲁棒水印解码器**（与模块3中嵌入`M`的网络相对应），该网络被训练用来抵抗各种降级。
  2. **纠错与分解：** 对解码器输出的原始比特流应用**前向纠错解码**（如BCH解码），修正可能存在的比特错误，得到干净的 `M_extracted`。然后根据预定格式，将其分割为 `c_bits_extracted` 和 `copyright_bits_extracted`。
  3. **重构语义向量 `c_reconstructed`：** 将 `c_bits_extracted` 输入到 **VQ-VAE的解码器**（与模块1中压缩模型对应），重构出高维语义向量。
  4. **本地生成基准水印 `W_base`：** 使用 `c_reconstructed` 作为密钥/种子，通过与嵌入端完全相同的**确定性伪随机数生成器**，在本地实时生成用于比对的、无损的基准定位水印 `W_base`。这一步是实现动态水印盲提取的“魔法”所在。
- **可参考的技术/论文：**
  - 此模块的逻辑是本方案的独创，但其组件（解码器、VQ-VAE）是标准技术。

#### **模块 6: 智能篡改定位**

- **目标：** 通过智能对比，输出不受噪声干扰的高精度篡改掩码，解决传统方法在低信噪比下降级伪影和篡改痕迹混淆的难题。
- **详细步骤：**
  1. **提取篡改后的水印 `W'_loc`：** 将 `I_rec` 输入到 **INN** 的**逆向过程**。此过程还需要一个辅助输入 `z`，这个 `z` 可以由一个专门的预测网络（参考 `EditGuard` 的Posterior Estimation Module）从 `I_rec` 中预测出来。最终解码出包含篡改痕迹和降级噪声的 `W'_loc`。
  2. **计算初步伪影图 `A_map`：** 计算 `A_map = |W'_loc - W_base|`。这张图谱高亮了所有不一致的区域，但无法区分原因。
  3. **使用智能提取器进行分析 (参考 `OmniGuard`)：**
     - **核心思想：** 简单的阈值处理无法区分篡改和噪声。我们需要一个更智能的判别器。
     - **输入：** 将待测图像 `I_rec` 和伪影图 `A_map` **同时**作为双通道（或多通道）输入，送入一个预训练好的、基于**Swin-Transformer**的检测网络。
     - **工作原理：** `A_map` 告诉网络“哪里可能有问题”，而 `I_rec` 提供了丰富的上下文信息，让网络判断“这个问题到底是什么”。例如，如果`A_map`在某处有信号，而`I_rec`在该处显示的是符合整体图像统计特性的JPEG压缩块效应，网络就会将其判定为噪声；反之，如果显示的是不协调的边缘或纹理，则判定为篡改。
     - **输出：** 该网络最终输出一个极为干净、准确的二值化篡改掩码 `M_loc`。
- **可参考的技术/论文：**
  - **篡改提取器：** **`OmniGuard`** 的“被动式、降级感知的篡改提取器”是实现此部分的最优选择。备选方案可参考 **`TAG-WM`** 的“密集变化区域检测器（DVRD）”。