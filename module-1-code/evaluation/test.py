"""
VQ-VAEæµ‹è¯•å’Œè¯„ä¼°è„šæœ¬
"""

import os
import sys
import torch
import torch.nn.functional as F
import argparse
import json
from typing import List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.vqvae_model import SemanticVQVAE
from utils.clip_utils import CLIPTextEncoder, compute_reconstruction_metrics
from training.config import load_config


class VQVAETester:
    """VQ-VAEæµ‹è¯•å™¨"""
    
    def __init__(self, model_path, config_path=None, device="cuda"):
        """
        åˆå§‹åŒ–VQ-VAEæµ‹è¯•å™¨

        Args:
            model_path: æ¨¡å‹æƒé‡è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        print("=" * 60)
        print("ğŸ§ª åˆå§‹åŒ–SAAH-WMæ¨¡å—ä¸€æµ‹è¯•å™¨")
        print("=" * 60)

        self.device = torch.device(device)
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        if config_path and os.path.exists(config_path):
            self.config = load_config(config_path)
            print(f"   âœ“ ä»æ–‡ä»¶åŠ è½½é…ç½®: {config_path}")
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            from training.config import get_default_config
            self.config = get_default_config()
            print("   âœ“ ä½¿ç”¨é»˜è®¤é…ç½®")

        # åˆ›å»ºæ¨¡å‹
        print("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
        self.model = SemanticVQVAE(
            input_dim=self.config.model.input_dim,
            latent_dim=self.config.model.latent_dim,
            num_embeddings=self.config.model.num_embeddings,
            commitment_cost=self.config.model.commitment_cost,
            decay=self.config.model.decay,
            dropout=self.config.model.dropout
        ).to(self.device)

        model_params = sum(p.numel() for p in self.model.parameters())
        print(f"   âœ“ æ¨¡å‹å‚æ•°æ•°é‡: {model_params:,}")

        # åŠ è½½æƒé‡
        print("ğŸ“¦ åŠ è½½æ¨¡å‹æƒé‡...")
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                if 'epoch' in checkpoint:
                    print(f"   âœ“ åŠ è½½æ£€æŸ¥ç‚¹ (Epoch {checkpoint['epoch']})")
                if 'best_val_loss' in checkpoint:
                    print(f"   âœ“ æœ€ä½³éªŒè¯æŸå¤±: {checkpoint['best_val_loss']:.6f}")
            else:
                self.model.load_state_dict(checkpoint)
            print(f"   âœ“ æˆåŠŸåŠ è½½æƒé‡: {model_path}")
        except Exception as e:
            print(f"   âŒ åŠ è½½æƒé‡å¤±è´¥: {e}")
            raise

        self.model.eval()

        # åˆå§‹åŒ–CLIPç¼–ç å™¨
        print("ğŸ”¤ åˆå§‹åŒ–CLIPç¼–ç å™¨...")
        try:
            self.clip_encoder = CLIPTextEncoder(device=device)
            print("   âœ“ CLIPç¼–ç å™¨å°±ç»ª")
        except Exception as e:
            print(f"   âŒ CLIPç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

        # è®¡ç®—å‹ç¼©å‚æ•°
        self.num_bits = (self.config.model.num_embeddings - 1).bit_length()
        compression_ratio = (self.config.model.input_dim * 32) / self.num_bits

        print("ğŸ“Š å‹ç¼©å‚æ•°:")
        print(f"   â€¢ ç æœ¬å¤§å°: {self.config.model.num_embeddings}")
        print(f"   â€¢ å‹ç¼©æ¯”ç‰¹æ•°: {self.num_bits}")
        print(f"   â€¢ å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
        print(f"   â€¢ å­˜å‚¨èŠ‚çœ: {(1 - self.num_bits / (self.config.model.input_dim * 32)) * 100:.2f}%")
        print("=" * 60)
    
    def prompt_to_bits(self, prompt: str) -> str:
        """
        å°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸ºäºŒè¿›åˆ¶ä¸²
        Args:
            prompt: æ–‡æœ¬æç¤º
        Returns:
            str: äºŒè¿›åˆ¶ä¸²
        """
        # è·å–CLIPå‘é‡
        clip_vector = self.clip_encoder.encode_text(prompt)
        
        # è·å–VQ-VAEç´¢å¼•
        with torch.no_grad():
            indices = self.model.encode(clip_vector)
        
        # è½¬æ¢ä¸ºäºŒè¿›åˆ¶
        index = indices[0].item()
        binary_str = format(index, f'0{self.num_bits}b')
        
        return binary_str
    
    def bits_to_reconstructed_vector(self, bits: str) -> torch.Tensor:
        """
        å°†äºŒè¿›åˆ¶ä¸²è½¬æ¢å›é‡æ„çš„è¯­ä¹‰å‘é‡
        Args:
            bits: äºŒè¿›åˆ¶ä¸²
        Returns:
            torch.Tensor: é‡æ„çš„è¯­ä¹‰å‘é‡
        """
        # è½¬æ¢ä¸ºç´¢å¼•
        index = int(bits, 2)
        indices = torch.tensor([index], device=self.device)
        
        # é‡æ„å‘é‡
        with torch.no_grad():
            reconstructed = self.model.decode_from_indices(indices)
        
        return reconstructed
    
    def test_single_prompt(self, prompt: str) -> Dict:
        """
        æµ‹è¯•å•ä¸ªæç¤ºçš„å®Œæ•´æµæ°´çº¿
        Args:
            prompt: æ–‡æœ¬æç¤º
        Returns:
            dict: æµ‹è¯•ç»“æœ
        """
        # åŸå§‹å‘é‡
        original_vector = self.clip_encoder.encode_text(prompt)
        
        # å‹ç¼©ä¸ºæ¯”ç‰¹ä¸²
        bits = self.prompt_to_bits(prompt)
        
        # é‡æ„å‘é‡
        reconstructed_vector = self.bits_to_reconstructed_vector(bits)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = compute_reconstruction_metrics(original_vector, reconstructed_vector)
        
        return {
            'prompt': prompt,
            'bits': bits,
            'index': int(bits, 2),
            'metrics': metrics,
            'original_vector': original_vector.cpu(),
            'reconstructed_vector': reconstructed_vector.cpu()
        }
    
    def test_consistency(self, prompt: str, num_tests: int = 10) -> Dict:
        """
        æµ‹è¯•é‡æ„çš„ä¸€è‡´æ€§
        Args:
            prompt: æ–‡æœ¬æç¤º
            num_tests: æµ‹è¯•æ¬¡æ•°
        Returns:
            dict: ä¸€è‡´æ€§æµ‹è¯•ç»“æœ
        """
        bits = self.prompt_to_bits(prompt)
        reconstructed_vectors = []
        
        # å¤šæ¬¡é‡æ„
        for _ in range(num_tests):
            reconstructed = self.bits_to_reconstructed_vector(bits)
            reconstructed_vectors.append(reconstructed.cpu())
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        first_vector = reconstructed_vectors[0]
        all_identical = all(
            torch.allclose(vec, first_vector, atol=1e-6) 
            for vec in reconstructed_vectors
        )
        
        # è®¡ç®—æœ€å¤§å·®å¼‚
        max_diff = 0
        for vec in reconstructed_vectors[1:]:
            diff = torch.max(torch.abs(vec - first_vector)).item()
            max_diff = max(max_diff, diff)
        
        return {
            'prompt': prompt,
            'bits': bits,
            'all_identical': all_identical,
            'max_difference': max_diff,
            'num_tests': num_tests
        }
    
    def test_codebook_coverage(self, test_prompts: List[str]) -> Dict:
        """
        æµ‹è¯•ç æœ¬è¦†ç›–ç‡
        Args:
            test_prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
        Returns:
            dict: ç æœ¬è¦†ç›–ç‡ç»“æœ
        """
        used_indices = set()
        
        for prompt in test_prompts:
            bits = self.prompt_to_bits(prompt)
            index = int(bits, 2)
            used_indices.add(index)
        
        coverage = len(used_indices) / self.config.model.num_embeddings
        
        return {
            'total_embeddings': self.config.model.num_embeddings,
            'used_embeddings': len(used_indices),
            'coverage_ratio': coverage,
            'used_indices': sorted(list(used_indices))
        }
    
    def benchmark_performance(self, test_prompts: List[str]) -> Dict:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        Args:
            test_prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
        Returns:
            dict: æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        import time
        
        # ç¼–ç æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        for prompt in test_prompts:
            self.prompt_to_bits(prompt)
        encoding_time = time.time() - start_time
        
        # è§£ç æ€§èƒ½æµ‹è¯•
        bits_list = [self.prompt_to_bits(prompt) for prompt in test_prompts]
        start_time = time.time()
        for bits in bits_list:
            self.bits_to_reconstructed_vector(bits)
        decoding_time = time.time() - start_time
        
        return {
            'num_prompts': len(test_prompts),
            'encoding_time': encoding_time,
            'decoding_time': decoding_time,
            'avg_encoding_time': encoding_time / len(test_prompts),
            'avg_decoding_time': decoding_time / len(test_prompts)
        }
    
    def comprehensive_test(self, test_prompts: List[str]) -> Dict:
        """
        ç»¼åˆæµ‹è¯•
        Args:
            test_prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
        Returns:
            dict: ç»¼åˆæµ‹è¯•ç»“æœ
        """
        results = {
            'model_info': {
                'num_embeddings': self.config.model.num_embeddings,
                'latent_dim': self.config.model.latent_dim,
                'num_bits': self.num_bits,
                'codebook_utilization': self.model.get_codebook_utilization()
            },
            'individual_tests': [],
            'consistency_tests': [],
            'codebook_coverage': None,
            'performance_benchmark': None,
            'summary_metrics': {}
        }
        
        # ä¸ªåˆ«æµ‹è¯•
        print("Running individual tests...")
        for prompt in test_prompts:
            result = self.test_single_prompt(prompt)
            results['individual_tests'].append(result)
        
        # ä¸€è‡´æ€§æµ‹è¯•ï¼ˆé€‰æ‹©å‰5ä¸ªæç¤ºï¼‰
        print("Running consistency tests...")
        for prompt in test_prompts[:5]:
            result = self.test_consistency(prompt)
            results['consistency_tests'].append(result)
        
        # ç æœ¬è¦†ç›–ç‡æµ‹è¯•
        print("Testing codebook coverage...")
        results['codebook_coverage'] = self.test_codebook_coverage(test_prompts)
        
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        print("Running performance benchmark...")
        results['performance_benchmark'] = self.benchmark_performance(test_prompts)
        
        # æ±‡æ€»æŒ‡æ ‡
        cos_sims = [test['metrics']['cosine_similarity'] for test in results['individual_tests']]
        mse_losses = [test['metrics']['mse_loss'] for test in results['individual_tests']]
        
        results['summary_metrics'] = {
            'avg_cosine_similarity': sum(cos_sims) / len(cos_sims),
            'min_cosine_similarity': min(cos_sims),
            'max_cosine_similarity': max(cos_sims),
            'avg_mse_loss': sum(mse_losses) / len(mse_losses),
            'all_consistent': all(test['all_identical'] for test in results['consistency_tests'])
        }
        
        return results


def main():
    parser = argparse.ArgumentParser(description="Test Semantic VQ-VAE")
    parser.add_argument("--model_path", required=True, help="Path to model checkpoint")
    parser.add_argument("--config_path", help="Path to config file")
    parser.add_argument("--device", default="cuda", help="Device to use")
    parser.add_argument("--output_dir", default="./test_results", help="Output directory")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è®¾å¤‡
    if args.device == "cuda" and not torch.cuda.is_available():
        print("CUDA not available, using CPU")
        args.device = "cpu"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = VQVAETester(args.model_path, args.config_path, args.device)
    
    # æµ‹è¯•æç¤º
    test_prompts = [
        "a cat sitting on a chair",
        "a beautiful sunset over the ocean",
        "a person riding a bicycle in the park",
        "a red car driving on the highway",
        "a dog playing with a ball",
        "a mountain landscape with snow",
        "a city skyline at night",
        "a flower garden in spring",
        "a bird flying in the sky",
        "a book on a wooden table"
    ]
    
    # è¿è¡Œç»¼åˆæµ‹è¯•
    print("Starting comprehensive test...")
    results = tester.comprehensive_test(test_prompts)
    
    # ä¿å­˜ç»“æœ
    results_path = os.path.join(args.output_dir, "test_results.json")
    with open(results_path, 'w') as f:
        # è½¬æ¢tensorä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_tensors(obj):
            if isinstance(obj, torch.Tensor):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_tensors(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_tensors(item) for item in obj]
            else:
                return obj
        
        json.dump(convert_tensors(results), f, indent=2)
    
    print(f"Test results saved to {results_path}")
    
    # æ‰“å°æ±‡æ€»
    print("\n=== Test Summary ===")
    print(f"Average cosine similarity: {results['summary_metrics']['avg_cosine_similarity']:.4f}")
    print(f"Minimum cosine similarity: {results['summary_metrics']['min_cosine_similarity']:.4f}")
    print(f"Average MSE loss: {results['summary_metrics']['avg_mse_loss']:.6f}")
    print(f"All reconstructions consistent: {results['summary_metrics']['all_consistent']}")
    print(f"Codebook coverage: {results['codebook_coverage']['coverage_ratio']:.3f}")
    print(f"Codebook utilization: {results['model_info']['codebook_utilization']:.3f}")


if __name__ == "__main__":
    main()
